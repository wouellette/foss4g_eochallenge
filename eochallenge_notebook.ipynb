{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports and other initializations\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# General imports \n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "from enum import Enum\n",
    "import datetime\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import easydict\n",
    "\n",
    "# Imports relative to web mapping and displaying results\n",
    "from ipyleaflet import Map, basemaps, ImageOverlay, TileLayer\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "from sidecar import Sidecar\n",
    "\n",
    "# Array processing libraries\n",
    "import numpy as np\n",
    "from numpy import unravel_index\n",
    "import rasterio as rio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "# Geospatial data wrangling (mostly vector data)\n",
    "import utm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# EO-learn/Sentinelhub imports\n",
    "from eolearn.core import (\n",
    "\tEOWorkflow,\n",
    "\tDependency,\n",
    "\tEOPatch,\n",
    "\tLoadFromDisk,\n",
    "\tSaveToDisk,\n",
    "\tOverwritePermission,\n",
    "\tFeatureType,\n",
    "\tLinearWorkflow,\n",
    "\tEOTask)\n",
    "from eolearn.features import LinearInterpolation, SimpleFilterTask\n",
    "from eolearn.geometry import PointSamplingTask, VectorToRaster\n",
    "from eolearn.io import ExportToTiff, SentinelHubWCSInput\n",
    "from eolearn.mask import AddValidDataMaskTask\n",
    "from sentinelhub import CRS, BBoxSplitter\n",
    "from eolearn.ml_tools import MorphologicalOperations, MorphologicalStructFactory\n",
    "\n",
    "# ML libraries\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "import shap\n",
    "\n",
    "# If a buffer size is defined around the AOI, it can be passed here in meters\n",
    "args = easydict.EasyDict({\n",
    "  \"aoi\": \"./aoi_eochallenge.shp\",\n",
    "  \"training_path\": [\"./trainingdata_germany.shp\",\"./trainingdata_romania.shp\"],\n",
    "  \"aoi_bufsize\": 10000,\n",
    "  \"zoom_level\": (10,10),\n",
    "  \"hour_diff\": 73,\n",
    "  \"cloud_threshold\": 0.8,\n",
    "  \"maxcc\": 0.5,\n",
    "  \"dest\": \"/path/to/dest\",\n",
    "  \"time_range\": [\"2005-01-01\",\"2019-08-01\"],\n",
    "  \"save_choice\": True,\n",
    "  \"n_procs\": 4,\n",
    "  \"resolution\": 23.5,\n",
    "  \"narrow_interpolation\": True,\n",
    "  \"proba\": True,\n",
    "  \"shap\": False,\n",
    "  \"gif\": True,\n",
    "  \"interpolation_interval\": 30,\n",
    "  \"lulc_classes\": {\n",
    "    \"no data\": 0,\n",
    "    \"forest\": 1,\n",
    "    \"grass\": 2,\n",
    "    \"cropland\": 3,\n",
    "    \"rock/soil\": 4,\n",
    "    \"built-up\": 5,\n",
    "    \"water\": 6,\n",
    "  }})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Various utilities required by the notebook\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MaxCCPredicate:\n",
    "    \"\"\"\n",
    "    Returns True if the image contains a smaller cloud cover percentage than the specified 'maxcc' value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, maxcc):\n",
    "        self.maxcc = maxcc\n",
    "\n",
    "    def __call__(self, img_cm):\n",
    "        w, h, _ = img_cm.shape\n",
    "        cc = np.sum(img_cm) / (w * h)\n",
    "        return cc <= self.maxcc\n",
    "\n",
    "\n",
    "class SentinelHubValidData:\n",
    "    \"\"\"\n",
    "    Combine Sen2Cor's classification map with `IS_DATA` to define a `VALID_DATA_SH` mask\n",
    "    The SentinelHub's cloud mask is assumed to be found in eopatch.mask['CLM']\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, eopatch):\n",
    "        return np.logical_and(eopatch.mask['IS_DATA'].astype(np.bool),\n",
    "                              np.logical_not(eopatch.mask['CLM'].astype(np.bool)))\n",
    "\n",
    "\n",
    "class CountValid(EOTask):\n",
    "    \"\"\"\n",
    "    The task counts number of valid observations in time-series and stores the results in the timeless mask.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, count_what, feature_name):\n",
    "        self.what = count_what\n",
    "        self.name = feature_name\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        eopatch.add_feature(FeatureType.MASK_TIMELESS, self.name, np.count_nonzero(eopatch.mask[self.what], axis=0))\n",
    "\n",
    "        return eopatch\n",
    "\n",
    "\n",
    "class MakeGif:\n",
    "    \"\"\"\n",
    "    Generates a GIF animation from an EOPatch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size=(640, 480)):\n",
    "        self.fig = plt.figure()\n",
    "        self.fig.set_size_inches(size[0] / 100, size[1] / 100)\n",
    "        ax = self.fig.add_axes([0, 0, 1, 1], frameon=False, aspect=1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        self.images = []\n",
    "\n",
    "    def add(self, eopatch, label='', band=1):\n",
    "        for i in range(len(eopatch)):\n",
    "            image = eopatch[i]\n",
    "            if image.shape[2] != 3 or image.shape[2] != 4:\n",
    "                image = image[..., band - 1]\n",
    "                plt_im = plt.imshow(image, cmap='Greys', animated=True)\n",
    "            else:\n",
    "                plt_im = plt.imshow(image, animated=True)\n",
    "            plt_txt = plt.text(10, 40, label[i], color='red')\n",
    "            self.images.append([plt_im, plt_txt])\n",
    "\n",
    "    def save(self, filename, fps):\n",
    "        animation = anim.ArtistAnimation(self.fig, self.images)\n",
    "        animation.save(filename, writer='imagemagick', fps=fps)\n",
    "\n",
    "\n",
    "class LoadFromMemory(EOTask):\n",
    "    \"\"\"\n",
    "        Task to load EOPatches to the EOWorkflow from memory\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def execute(eopatch):\n",
    "        return eopatch\n",
    "\n",
    "\n",
    "class ConcatenateData(EOTask):\n",
    "    \"\"\"\n",
    "        Task to concatenate data arrays along the last dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_name, feature_names_to_concatenate):\n",
    "        self.feature_name = feature_name\n",
    "        self.feature_names_to_concatenate = feature_names_to_concatenate\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        arrays = [eopatch.data[name] for name in self.feature_names_to_concatenate]\n",
    "\n",
    "        eopatch.add_feature(FeatureType.DATA, self.feature_name, np.concatenate(arrays, axis=-1))\n",
    "\n",
    "        return eopatch\n",
    "\n",
    "\n",
    "class MoveFeature(EOTask):\n",
    "    \"\"\"\n",
    "    Task to copy fields from one eopatch to another\n",
    "    The fields are defined as a dictionary in the form {FeatureType: {feature_names_to_copy}}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fields_to_copy):\n",
    "        self.fields_to_copy = fields_to_copy\n",
    "\n",
    "    def execute(self, *eopatches):\n",
    "\n",
    "        dst_eopatch, src_eopatch = eopatches\n",
    "        for key in self.fields_to_copy.keys():\n",
    "            for name in self.fields_to_copy[key]:\n",
    "                dst_eopatch.add_feature(key, name, src_eopatch[key][name])\n",
    "        return dst_eopatch\n",
    "\n",
    "\n",
    "class ClassFilterTask(EOTask):\n",
    "    \"\"\"\n",
    "    Run class specific morphological operation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lulc_feature, lulc_values, morph_operation, struct_elem=None):\n",
    "        self.lulc_feature_type, self.lulc_feature_name = next(iter(self._parse_features(lulc_feature)))\n",
    "        self.lulc_values = lulc_values\n",
    "\n",
    "        if isinstance(morph_operation, MorphologicalOperations):\n",
    "            self.morph_operation = MorphologicalOperations.get_operation(morph_operation)\n",
    "        else:\n",
    "            self.morph_operation = morph_operation\n",
    "        self.struct_elem = struct_elem\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        lulc = eopatch[self.lulc_feature_type][self.lulc_feature_name].copy()\n",
    "\n",
    "        for lulc_value in self.lulc_values:\n",
    "            lulc_mod = self.morph_operation((lulc == lulc_value).squeeze(), self.struct_elem) * lulc_value\n",
    "            lulc_mod = lulc_mod[..., np.newaxis]\n",
    "            lulc[lulc == lulc_value] = lulc_mod[lulc == lulc_value]\n",
    "\n",
    "        eopatch.add_feature(self.lulc_feature_type, self.lulc_feature_name, lulc)\n",
    "\n",
    "        return eopatch\n",
    "\n",
    "\n",
    "def recompute_interpolation_range(range_idx, out_path=None, n_samples=1000, **kwargs):\n",
    "    \"\"\"\n",
    "    Recompute the \"start_date\" and \"end_date\" of a time range based on a set of EOPatches\n",
    "    \"\"\"\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = '/'.join([str(arg) for arg in kwargs.values()])\n",
    "\n",
    "    # load sampled eopatches\n",
    "    eopatches = []\n",
    "    eopatches_sample = []\n",
    "    for idx in range_idx:\n",
    "        if os.path.isfile(f'{out_path}/lulc_sample/eopatch_{idx}/data/BANDS.npy') \\\n",
    "                or os.path.isfile(f'{out_path}/lulc_nosample/eopatch_{idx}/data/BANDS.npy'):\n",
    "            try:\n",
    "                eopatches.append(EOPatch.load(f'{out_path}/lulc_sample/eopatch_{idx}', lazy_loading=True))\n",
    "                eopatches_sample.append(EOPatch.load(f'{out_path}/lulc_sample/eopatch_{idx}', lazy_loading=True))\n",
    "            except:\n",
    "                eopatches.append(EOPatch.load(f'{out_path}/lulc_nosample/eopatch_{idx}', lazy_loading=True))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    eopatches = np.asarray(eopatches)\n",
    "\n",
    "    if n_samples is None:\n",
    "        n_samples = np.amin([np.count_nonzero(eopatch.mask_timeless['LULC']) for eopatch in eopatches_sample])\n",
    "\n",
    "    print(f'n_samples:{n_samples}')\n",
    "\n",
    "    start_date = np.amax([eopatch.timestamp[0] for eopatch in eopatches]).strftime(\"%Y-%m-%d\")\n",
    "    end_date = np.amin([eopatch.timestamp[-1] for eopatch in eopatches]).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    print(f'start date of interpolation range:{start_date}')\n",
    "    print(f'end date of interpolation range:{end_date}')\n",
    "\n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "def clean_training_data(range_sample, out_path=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Performs cleaning on the EOPatches which contain training data.\n",
    "    The types of errors it corrects are the following:\n",
    "    - Training data polygon slivers inside the EOPatch which, when rasterized, return 0 training pixels.\n",
    "    - Erroneous training data vector geometries\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    range_sample: list of integer\n",
    "       A list of EOPatch indices indicating the EOPatches that contain training data\n",
    "    out_path: str\n",
    "        Path where to save generated outputs\n",
    "    **kwargs: dict\n",
    "        Other arguments which may be specified to create a custom output path to store outputs\n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "    eopatches: list of EOPatch Object\n",
    "        Lazily loaded list of EOPatches\n",
    "    range_sample_clean: list of integer\n",
    "        Cleaned-up list of EOPatch indices\n",
    "    \"\"\"\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = '/'.join([str(arg) for arg in kwargs.values()])\n",
    "\n",
    "    eopatches = []\n",
    "    range_sample_clean = []\n",
    "    for idx in range_sample:\n",
    "        if os.path.isdir(f'{out_path}/lulc_sampled/eopatch_{idx}'):\n",
    "            eopatch = EOPatch.load(f'{out_path}/lulc_sampled/eopatch_{idx}', lazy_loading=True)\n",
    "        else:\n",
    "            continue\n",
    "        try:\n",
    "            if eopatch.mask_timeless['LULC_SAMPLED'].shape[0] == 0:\n",
    "                print(f'patch {idx} removed from samples due to insufficient amount of samples, removing it...')\n",
    "                eopatch.remove_feature(FeatureType.MASK_TIMELESS, 'LULC_SAMPLED', )\n",
    "                eopatch.remove_feature(FeatureType.DATA_TIMELESS, 'SPLIT_SAMPLED', )\n",
    "            else:\n",
    "                print(f'patch {idx} kept in sample')\n",
    "                range_sample_clean.append(idx)\n",
    "                eopatches.append(eopatch)\n",
    "        except:\n",
    "            print(f'patch {idx} removed from samples due to unknown defect...')\n",
    "\n",
    "    return eopatches, range_sample_clean\n",
    "\n",
    "def _craft_input_features(eopatches, range_sample, lulc_classes, out_path=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Prepare the numpy arrays input features necessary to train and test the Random Forest model\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    range_sample: list of integer\n",
    "       A list of EOPatches indices indicating the EOPatches that contain training data\n",
    "    class_labels: list of integer\n",
    "       List of integers indicating the land cover classes to be classified\n",
    "    class_names: list of string\n",
    "        List of strings (land cover class names) indicating the land cover classes to be classified\n",
    "    out_path: str\n",
    "        Path where to save generated outputs\n",
    "    **kwargs: dict\n",
    "        Other arguments which may be specified to create a custom output path to store outputs\n",
    "\n",
    "    Returns\n",
    "    ------------\n",
    "    features_train: numpy array\n",
    "        Numpy array containing input features to train on\n",
    "    features_test: numpy array\n",
    "        Numpy array containing input features to test on\n",
    "    labels_train: numpy array\n",
    "        Numpy array containing input labels to train on\n",
    "    labels_test: numpy array\n",
    "        Numpy array containing input labels to test on\n",
    "    labels_unique: list of integer\n",
    "        List of integers identifying the labels which are effectively present in the training dataset\n",
    "        (i.e. some classes maybe missing from the full land cover class nomenclature)\n",
    "    \"\"\"\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = '/'.join([str(arg) for arg in kwargs.values()])\n",
    "\n",
    "    if not os.path.isdir(f'{out_path}/validation'):\n",
    "        os.makedirs(f'{out_path}/validation', exist_ok=True)\n",
    "    joblib.dump(range_sample, f'{out_path}/validation/range_sample.pkl')\n",
    "    random.shuffle(range_sample)\n",
    "\n",
    "    split = np.stack([eopatch.data_timeless['SPLIT_SAMPLED'] for eopatch in eopatches])\n",
    "    labels = np.stack([eopatch.mask_timeless['LULC_SAMPLED'] for eopatch in eopatches])\n",
    "\n",
    "    labels_train = np.where(split == 1, labels, 0)\n",
    "    labels_test = np.where(split == 2, labels, 0)\n",
    "\n",
    "    features = np.stack([eopatch.data['FEATURES_SAMPLED'] for eopatch in eopatches])\n",
    "    features_reshape = np.tile(split, (features.shape[1], 1, 1, features.shape[4]))\n",
    "    features = np.concatenate(features)\n",
    "    features_train = np.where(features_reshape == 1, features, 0).reshape(len(range_sample),\n",
    "                                                                          int(features.shape[0] / len(range_sample)),\n",
    "                                                                          features.shape[1], features.shape[2],\n",
    "                                                                          features.shape[3])\n",
    "    features_test = np.where(features_reshape == 2, features, 0).reshape(len(range_sample),\n",
    "                                                                         int(features.shape[0] / len(range_sample)),\n",
    "                                                                         features.shape[1], features.shape[2],\n",
    "                                                                         features.shape[3])\n",
    "\n",
    "    print(np.count_nonzero(labels))\n",
    "    print(np.count_nonzero(split))\n",
    "    print(np.unique(labels_train))\n",
    "    print(np.unique(labels_test))\n",
    "\n",
    "    # rotate dimension so that reshape can be used\n",
    "    p1, t1, w1, h1, f1 = features_train.shape\n",
    "    p2, t2, w2, h2, f2 = features_test.shape\n",
    "\n",
    "    # reshape to n x m\n",
    "    features_train = np.swapaxes(features_train, 1, 3).reshape(p1 * h1 * w1, t1 * f1)\n",
    "    labels_train = np.swapaxes(labels_train, 1, 2).reshape(p1 * h1 * w1, 1).squeeze()\n",
    "    features_test = np.swapaxes(features_test, 1, 3).reshape(p2 * h2 * w2, t2 * f2)\n",
    "    labels_test = np.swapaxes(labels_test, 1, 2).reshape(p2 * h2 * w2, 1).squeeze()\n",
    "\n",
    "    t = open(f'{out_path}/validation/training_test_sample_size.txt', 'w+')\n",
    "    for cl_name, cl_type in lulc_classes.items():\n",
    "        print(f'number of training samples for: {cl_name}')\n",
    "        print(len(labels_train[labels_train == cl_type]))\n",
    "        t.write(f'number of training samples for: {cl_name}\\n')\n",
    "        t.write(f'{len(labels_train[labels_train == cl_type])}\\n')\n",
    "        print(f'number of test samples for: {cl_name}')\n",
    "        print(len(labels_test[labels_test == cl_type]))\n",
    "        t.write(f'number of test samples for: {cl_name}\\n')\n",
    "        t.write(f'{len(labels_test[labels_test == cl_type])}\\n')\n",
    "    t.close()\n",
    "\n",
    "\n",
    "\n",
    "    print(labels_train.shape)\n",
    "    print(features_train.shape)\n",
    "    print(labels_test.shape)\n",
    "    print(features_test.shape)\n",
    "\n",
    "    # TODO: Perform independent validation, different from test?\n",
    "    # features_train, features_test, labels_train, labels_test = train_test_split(features_train, labels_train, test_size=0.3, shuffle=True, random_state=42)\n",
    "    # features_valid, features_test, labels_valid, labels_test = train_test_split(features_test, labels_test,\n",
    "    #                                                                            test_size=0.7, shuffle=True,\n",
    "    #                                                                            random_state=42)\n",
    "\n",
    "    # remove points with no reference from training (so we dont train to recognize \"no data\")\n",
    "    mask = labels_train == 0\n",
    "    features_train = features_train[~mask]\n",
    "    labels_train = labels_train[~mask]\n",
    "\n",
    "    # remove points with no reference from test (so we dont validate on \"no data\", which doesn't make sense)\n",
    "    mask = labels_test == 0\n",
    "    features_test = features_test[~mask]\n",
    "    labels_test = labels_test[~mask]\n",
    "\n",
    "    # Set up training classes\n",
    "    labels_unique = np.unique(labels_train)\n",
    "    print(np.unique(labels_test))\n",
    "    print(np.unique(labels_train))\n",
    "\n",
    "    return range_sample, features_train, features_test, labels_train, labels_test, labels_unique, p1, t1, w1, h1, f1\n",
    "\n",
    "class LULC(Enum):\n",
    "    NO_DATA = (0, 'No Data', 'white')\n",
    "    WOODY = (1, 'Forest', 'xkcd:darkgreen')\n",
    "    GRASS = (2, 'Grassland', 'xkcd:grass')\n",
    "    CROPLAND = (3, 'Cultivated Land', 'xkcd:maize')\n",
    "    ROCK_SOIL = (4, 'Bareland', 'xkcd:marigold')\n",
    "    BUILTUP = (5, 'Artificial Surface', 'xkcd:red')\n",
    "    WATER = (6, 'Permanent Water', 'xkcd:blue')\n",
    "\n",
    "    def __init__(self, val1, val2, val3):\n",
    "        self.id = val1\n",
    "        self.class_name = val2\n",
    "        self.color = val3\n",
    "\n",
    "class PredictPatch(EOTask):\n",
    "    \"\"\"\n",
    "    The task counts number of valid observations in time-series and stores the results in the timeless mask.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, n_classes, proba=False, shap=False):\n",
    "        self.model = model\n",
    "        self.n_classes = n_classes\n",
    "        self.proba = proba\n",
    "        self.shap = shap\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        feature = eopatch.data['FEATURES']\n",
    "        t, w, h, f = feature.shape\n",
    "        feature = np.swapaxes(feature, 0, 2).reshape(h * w, t * f)\n",
    "\n",
    "        # TODO: see how to use SHAP\n",
    "        # TODO: Add Jira Ticket\n",
    "        if self.shap is True:\n",
    "            plabels_shap = self.model.predict(feature, pred_contrib=True)\n",
    "            eopatch.add_feature(FeatureType.SCALAR, 'PRED_SHAP', plabels_shap)\n",
    "\n",
    "        plabels = self.model.predict(feature)\n",
    "        plabels = np.swapaxes(plabels.reshape(h, w), 0, 1)\n",
    "        plabels = plabels[..., np.newaxis]\n",
    "        eopatch.add_feature(FeatureType.DATA_TIMELESS, 'PRED', plabels)\n",
    "\n",
    "        if self.proba is True:\n",
    "            plabels_proba = self.model.predict_proba(feature)\n",
    "            plabels_proba = np.swapaxes(plabels_proba.reshape(h, w, self.n_classes), 0, 1)\n",
    "            eopatch.add_feature(FeatureType.DATA_TIMELESS, 'PRED_PROBA', plabels_proba)\n",
    "\n",
    "        return eopatch\n",
    "    \n",
    "def intersect_aoi(bbox_splitter, training):\n",
    "\n",
    "    geometry = [Polygon(bbox.get_polygon()) for bbox in bbox_splitter.bbox_list]\n",
    "    idxs_x = [info['index_x'] for info in bbox_splitter.info_list]\n",
    "    idxs_y = [info['index_y'] for info in bbox_splitter.info_list]\n",
    "    df = pd.DataFrame({'index_x': idxs_x, 'index_y': idxs_y})\n",
    "    gdf = gpd.GeoDataFrame(df, crs={'init': CRS.ogc_string(bbox_splitter.bbox_list[0].crs)}, geometry=geometry)\n",
    "    gdf.head()\n",
    "    range_bool = gdf.intersects(training.buffer(0).unary_union)\n",
    "    bbox_train_list = []\n",
    "    for i in range(len(range_bool)):\n",
    "        if range_bool[i]:\n",
    "            bbox_train_list.append(bbox_splitter.bbox_list[i])\n",
    "    return bbox_train_list\n",
    "    \n",
    "def multiprocess(n_procs, range_idx, func):\n",
    "    \"\"\"\n",
    "    Function to enable multiprocessing of the different subpackages' calls\n",
    "    \"\"\"\n",
    "    p = Pool(n_procs, maxtasksperchild=1)\n",
    "    p.map(func, range_idx, chunksize=1)\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load AOI geometry and split it into a processing grid.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The AOI should be in EPSG:4326\n",
    "out_path = args.dest\n",
    "bufsize = args.aoi_bufsize\n",
    "row = args.zoom_level[0]\n",
    "col = args.zoom_level[1]\n",
    "\n",
    "aoi = gpd.read_file(args.aoi)\n",
    "aoi['geometry'] = aoi['geometry'].to_crs(epsg=3857).buffer(bufsize).to_crs(epsg=4326)\n",
    "\n",
    "bbox_splitter_list = []\n",
    "# Assuming there are two AOIs: one for Romania, one for Germany\n",
    "for idx, aoi_el in zip(['Germany','Romania'], aoi.geometry.values.tolist()):\n",
    "    \n",
    "    aoi_latlon = aoi_el[0].centroid.coords[0]\n",
    "    utm_crs = utm.from_latlon(aoi_latlon[1], aoi_latlon[0])\n",
    "    if bbox_latlon[1] > 0:\n",
    "        if utm_crs[2] > 9:\n",
    "            aoi_crs = 'EPSG:326%s' % utm_crs[2]\n",
    "        else:\n",
    "            aoi_crs = 'EPSG:3260%s' % utm_crs[2]\n",
    "    else:\n",
    "        if utm_crs[2] > 9:\n",
    "            aoi_crs = 'EPSG:327%s' % utm_crs[2]\n",
    "        else:\n",
    "            aoi_crs = 'EPSG:3270%s' % utm_crs[2]\n",
    "            \n",
    "    # Split the AOI into a processing grid matching the OSM zoom level 12 grid\n",
    "    bbox_splitter = BBoxSplitter([aoi_el], aoi_crs, (row, col))\n",
    "    bbox_splitter_list.append(bbox_splitter)\n",
    "    \n",
    "    bbox_list = bbox_splitter.get_bbox_list()\n",
    "    print('Area bounding box: {}\\n'.format(bbox_splitter.get_area_bbox().__repr__()))\n",
    "    print('Each bounding box also has some info how it was created. Example:'\n",
    "          '\\nbbox: {}\\ninfo: {}\\n'.format(bbox_list[0].__repr__(), bbox_splitter.info_list[0]))\n",
    "    print(f'\\nThe AOI is covered by {len(bbox_splitter.bbox_list)} tiles to be processed.')\n",
    "\n",
    "    # Create the path where to store the split processing grid.\n",
    "    os.makedirs(f'{out_path}/aoi', exist_ok=True)\n",
    "\n",
    "    with open(f'{out_path}/aoi/aoi_bbox_4326_{idx}_r{row}_c{col}.pkl', 'wb') as f:\n",
    "        pickle.dump(bbox_splitter, f)\n",
    "\n",
    "    geometry = [Polygon(bbox.get_polygon()) for bbox in bbox_list]\n",
    "    idxs_x = [info['index_x'] for info in bbox_splitter.info_list]\n",
    "    idxs_y = [info['index_y'] for info in bbox_splitter.info_list]\n",
    "\n",
    "    df = pd.DataFrame({'index_x': idxs_x, 'index_y': idxs_y})\n",
    "    gdf = gpd.GeoDataFrame(df, crs={'init': CRS.ogc_string(bbox_list[0].crs)}, geometry=geometry)\n",
    "    gdf.head()\n",
    "\n",
    "    gdf.to_file(f'{out_path}/aoi/aoi_bbox_4326_{idx}_r{row}_c{col}_{len(bbox_splitter.bbox_list)}.shp')\n",
    "    \n",
    "    # Plot the processing grid produced on ipyleaflet\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split Training dataset into a training and test parts\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "training_romania = gpd.read_file(args.training_path[1])\n",
    "training_romania.head()\n",
    "training_germany = gpd.read_file(args.training_path[0])\n",
    "training_germany.head()\n",
    "\n",
    "trainings = [training_germany, training_romania]\n",
    "\n",
    "training_arrays = []\n",
    "split_arrays = []\n",
    "training_vals = []\n",
    "for idx, training in zip(['Germany','Romania'], trainings):\n",
    "    # List land cover class labels present in AOI\n",
    "    # training_utm = training.to_crs(crs={'init': CRS.ogc_string(aoi_crs)})\n",
    "    training_val = sorted(list(set(training.lulc_type)))\n",
    "    print(f'land cover labels present in AOI {idx}: {training_val}')\n",
    "    training = training.sample(frac=1, axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Calculate the summed area of training data per class.\n",
    "    # Assign 80 % of land cover class surface area to training, and remaining 20% to test\n",
    "    area_sum = 0\n",
    "    split_array = []\n",
    "    training_array =[]\n",
    "    for val in training_val:\n",
    "        row_area = 0\n",
    "        temp = training[training.lulc_type == val]\n",
    "        temp = temp.assign(area=temp['geom'].area / 1000000, split=0)\n",
    "        total_area = temp['area'].sum()\n",
    "        # print(f'total_area:{total_area}')\n",
    "        for irow in range(len(temp)):\n",
    "            row_area = row_area + temp['area'].iloc[[irow]].values[0]\n",
    "            # print(f'row_area:{row_area}')\n",
    "            if row_area < 0.8 * total_area and irow < len(temp.index) - 1:\n",
    "                temp.iloc[irow, -1] = 1\n",
    "            else:\n",
    "                temp.iloc[irow, -1] = 2\n",
    "\n",
    "        temp.reset_index(drop=True, inplace=True)\n",
    "        training_array.append(temp.drop(columns=['split', 'area']))\n",
    "        temp['lulc_type'] = temp['split']\n",
    "        split_array.append(temp.drop(columns=['split', 'area']))\n",
    "        area_sum = area_sum + total_area\n",
    "        del temp\n",
    "\n",
    "    # Create an identical pandas.dataframe to \"training_array\" labelling the data as training (1) or test (2).\n",
    "    split_array = pd.concat(split_array)\n",
    "    split_array.reset_index(drop=True, inplace=True)\n",
    "    split_array = [split_array[split_array['lulc_type'] == 1], split_array[split_array['lulc_type'] == 2]]\n",
    "    # pd.concat(split_array).plot(column='lulc_type',legend=True,figsize=(8,4))\n",
    "    # pd.concat(training_array).plot(column='lulc_type',legend=True,figsize=(8,4))\n",
    "\n",
    "    #os.makedirs(f'{out_path}/training_data', exist_ok=True)\n",
    "\n",
    "    # Save outputs to pickle files\n",
    "    # with open(f'{out_path}/training_data/training_array_{idx}.pkl', 'wb') as training_file:\n",
    "    #     pickle.dump(training_array, training_file)\n",
    "    # with open(f'{out_path}/training_data/split_array_{idx}.pkl', 'wb') as split_file:\n",
    "    #     pickle.dump(split_array, split_file)\n",
    "    # with open(f'{out_path}/training_data/training_val_{idx}.pkl', 'wb') as val_file:\n",
    "    #     pickle.dump(training_val, val_file)\n",
    "\n",
    "    training_arrays.append(training_array)\n",
    "    split_arrays.append(split_array)\n",
    "    training_vals.append(training_val)\n",
    "    \n",
    "    # Plot the training data distribution on ipyleaflet\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Satellite Imagery Collections\n",
    "\n",
    "TODOs: Check if the cloud detection routine is necessary, and if so, adapt it to work to LIS-III.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_eopatch(bbox_splitters, time_interval, training_array, split_array, training_val, out_path, bbox_idx,\n",
    "                 interp_interval=30, save_choice=True, cloud_threshold=0.4, maxcc=0.8, row=10, col=10,\n",
    "                 hour_diff=73, resolution=10):\n",
    "    \n",
    "    global patch_s2\n",
    "    start_date = time_interval[0]\n",
    "    end_date = time_interval[1]\n",
    "    eopatches = []\n",
    "    \n",
    "    for bbox in bbox_splitter.bbox_list:\n",
    "        if bbox.__eq__(bbox_idx):\n",
    "\n",
    "            idx = bbox_splitter.bbox_list.index(bbox)\n",
    "            \n",
    "            print(info)\n",
    "            print(f'bbox index to process:')\n",
    "            print(f'uuid: {idx}')\n",
    "            print(f'bbox splitter dimensions: {row}x{col} ')\n",
    "            print(f'index_x: {info[\"index_x\"]}')\n",
    "            print(f'index_y: {info[\"index_y\"]}')\n",
    "            \n",
    "            # While loop necessary to re-perform requests with different parameters.\n",
    "            # One case is if too little data is available with current parameters.\n",
    "            # Other case is if a HTTPRequestError (simply retry) or MemoryError (not currently handled) is encountered.\n",
    "            attempts = 0\n",
    "            while attempts < 5:\n",
    "                # TASK FOR BAND DATA\n",
    "                # 1. add a request for B(B02), G(B03), R(B04), NIR (B08), SWIR1(B11), SWIR2(B12)\n",
    "                # from default layer 'ALL_BANDS' at 10m resolution\n",
    "                custom_script = 'return [B02, B03, B04, B05, B06, B07, B08, B8A, B11, B12];'\n",
    "                \n",
    "                add_r2 = SentinelHubWCSInput(\n",
    "                    layer='FOSS4G_R2',\n",
    "                    instance_id = '1cad1239-abdf-4dc5-ae79-fe129b926ae2',\n",
    "                    feature=(FeatureType.DATA, 'R2'),\n",
    "                    resx=f'{resolution}m',  # resolution x\n",
    "                    resy=f'{resolution}m',  # resolution y\n",
    "                    maxcc=maxcc,\n",
    "                    time_difference=datetime.timedelta(hours=hour_diff)   \n",
    "                )\n",
    "                \n",
    "                add_p6 = SentinelHubWCSInput(\n",
    "                    layer='FOSS4G_P6',\n",
    "                    instance_id = '1cad1239-abdf-4dc5-ae79-fe129b926ae2',\n",
    "                    feature=(FeatureType.DATA, 'P6'),\n",
    "                    resx=f'{resolution}m',  # resolution x\n",
    "                    resy=f'{resolution}m',  # resolution y\n",
    "                    maxcc=maxcc,\n",
    "                    time_difference=datetime.timedelta(hours=hour_diff)\n",
    "                )\n",
    "                \n",
    "                # add_s2 = S2L1CWCSInput(\n",
    "                #     layer='BANDS-S2-L1C',\n",
    "                #     instance_id = '75afd1bc-8dba-45c3-a88a-782d39066bc3',\n",
    "                #     feature=(FeatureType.DATA, 'BANDS'),  # save under name 'BANDS'\n",
    "                #     custom_url_params={CustomUrlParam.EVALSCRIPT: custom_script},  # custom url for 6 specific bands\n",
    "                #     resx=f'{resolution}m',  # resolution x\n",
    "                #     resy=f'{resolution}m',  # resolution y\n",
    "                #     maxcc=maxcc,  # maximum allowed cloud cover of original ESA tiles\n",
    "                #     time_difference=datetime.timedelta(hours=hour_diff))  # time difference to consider to merge consecutive images\n",
    "            \n",
    "                # 2. Run SentinelHub's cloud detector\n",
    "                # (cloud detection is performed at 160m resolution\n",
    "                # and the resulting cloud probability map and mask\n",
    "                # are upscaled to EOPatch's resolution)\n",
    "                # Check whether this is really necessary or not, and if so, train new cloud detection model\n",
    "                # cloud_classifier = get_s2_pixel_cloud_detector(threshold=cloud_threshold,\n",
    "                #                                                average_over=2,\n",
    "                #                                                dilation_size=1,\n",
    "                #                                                all_bands=False)\n",
    "                # add_clm = AddCloudMaskTask(cloud_classifier,\n",
    "                #                            'BANDS-S2CLOUDLESS',\n",
    "                #                            cm_size_y='160m',\n",
    "                #                            cm_size_x='160m',\n",
    "                #                            cmask_feature='CLM',\n",
    "                #                            cprobs_feature='CLP')\n",
    "            \n",
    "                # Request Sentinel-1 Data Raw bands.\n",
    "                # add_S1 = SentinelHubWCSInput(\n",
    "                #   layer='BANDS-S1-IW',\n",
    "                #   data_source=DataSource.SENTINEL1_IW,\n",
    "                #   resx=f'{resolution}m',\n",
    "                #   resy=f'{resolution}m',\n",
    "                #   time_difference=hour_diff)\n",
    "            \n",
    "                # Request Sentinel-1 Data RGB False Color band combination.\n",
    "                # add_S1_truecolor = SentinelHubWCSInput(\n",
    "                #   layer='TRUE-COLOR-S1-IW',\n",
    "                #   data_source=DataSource.SENTINEL1_IW,\n",
    "                #   time_difference=hour_diff)\n",
    "            \n",
    "                # TASKS FOR CALCULATING NEW FEATURES\n",
    "                # NDVI: (B08 - B04)/(B08 + B04)\n",
    "                # NDWI: (B03 - B08)/(B03 + B08)\n",
    "                # NORM: sqrt(B02^2 + B03^2 + B04^2 + B08^2 + B11^2 + B12^2)\n",
    "            \n",
    "                # 3. Add additional band combinations like ndvi, ndwi and euclidian Norm\n",
    "                # add_ndvi = S2L1CWCSInput(\n",
    "                #    layer='NDVI',\n",
    "                #    time_difference=datetime.timedelta(hours=hour_diff))\n",
    "                # add_ndwi = S2L1CWCSInput(\n",
    "                #    layer='NDWI',\n",
    "                #    time_difference=datetime.timedelta(hours=hour_diff))\n",
    "            \n",
    "                # 4. Filter task to be used for gif generation (a gif with clouds isn't very pretty, nor useful)\n",
    "                filter_task_clm = SimpleFilterTask((FeatureType.MASK, 'CLM'), MaxCCPredicate(maxcc=0.05))\n",
    "            \n",
    "                # 5. Validate pixels using SentinelHub's cloud detection mask\n",
    "                add_sh_valmask = AddValidDataMaskTask(SentinelHubValidData(), 'IS_VALID')\n",
    "            \n",
    "                # If using multiple datasets, they may need to be coregistered to one another.\n",
    "                # s2_temporal_coregistration = ThunderRegistration((FeatureType.DATA,index))\n",
    "                # s1_temporal_coregistration = ThunderRegistration((FeatureType.DATA,'BANDS-S1-IW'))\n",
    "            \n",
    "                # 6. Count number of valid observations per pixel using valid data mask\n",
    "                count_val_sh = CountValid('IS_VALID', 'VALID_COUNT_SH')\n",
    "                rshape = (FeatureType.MASK, 'IS_VALID')\n",
    "            \n",
    "                # 7. Prepare task to burn training data and its training/test split into the EOPatch\n",
    "                training_task_array = []\n",
    "                for el, val in zip(training_array, training_val):\n",
    "                    training_task_array.append(VectorToRaster(\n",
    "                        raster_feature=(FeatureType.MASK_TIMELESS, 'LULC'),\n",
    "                        vector_input=el,\n",
    "                        values=val,\n",
    "                        raster_shape=rshape))\n",
    "            \n",
    "                split_task_array = []\n",
    "                for el, val in zip(split_array, [1, 2]):\n",
    "                    split_task_array.append(VectorToRaster(\n",
    "                        raster_feature=(FeatureType.DATA_TIMELESS, 'SPLIT'),\n",
    "                        vector_input=el,\n",
    "                        values=val,\n",
    "                        raster_shape=rshape))\n",
    "            \n",
    "                # 8. Export valid pixel count to tiff file\n",
    "                export_val_sh = ExportToTiff((FeatureType.MASK_TIMELESS, 'VALID_COUNT_SH'))\n",
    "            \n",
    "                extra_params = {}\n",
    "                extra_params[export_val_sh] = {'filename':f'/{out_path}/valid_{idx}_row-{info[\"index_x\"]}_col-{info[\"index_y\"]}.tiff'}\n",
    "            \n",
    "                # 9. define additional parameters of the workflow\n",
    "                extra_params[add_r2] = {'bbox': bbox, 'time_interval': time_interval}\n",
    "                extra_params[add_p6] = {'bbox': bbox, 'time_interval': time_interval}\n",
    "                #extra_params[add_s2] = {'bbox': bbox, 'time_interval': time_interval}\n",
    "            \n",
    "                # 9. Declare workflow\n",
    "                if gif:\n",
    "                    workflow = LinearWorkflow(\n",
    "                    add_r2,\n",
    "                    add_p6,\n",
    "                    #add_s2,\n",
    "                    #add_clm,\n",
    "                    #filter_task_clm,\n",
    "                    add_sh_valmask,\n",
    "                    count_val_sh,\n",
    "                    export_val_sh,\n",
    "                    *training_task_array,\n",
    "                    *split_task_array\n",
    "                    )\n",
    "                else:\n",
    "                    workflow = LinearWorkflow(\n",
    "                    add_r2,\n",
    "                    add_p6,\n",
    "                    #add_s2,\n",
    "                    #add_clm,\n",
    "                    filter_task_clm,\n",
    "                    add_sh_valmask,\n",
    "                    count_val_sh,\n",
    "                    export_val_sh,\n",
    "                    *training_task_array,\n",
    "                    *split_task_array\n",
    "                    )\n",
    "            \n",
    "                os.makedirs(f'{out_path}/lulc_sample', exist_ok=True)\n",
    "                os.makedirs(f'{out_path}/lulc_nosample', exist_ok=True)\n",
    "                \n",
    "               # Check if EOPatch already exists. If so, load it and skip workflow execution.\n",
    "                if os.path.isfile(f'{out_path}/lulc_sample/eopatch_{idx}/timestamp.pkl'):\n",
    "                    print(\"this file has already been generated, skipping to next patch\")\n",
    "                    patch_s2 = EOPatch.load(f'{out_path}/lulc_sample/eopatch_{idx}', lazy_loading=True)\n",
    "                    break\n",
    "                elif os.path.isfile(f'{out_path}/lulc_nosample/eopatch_{idx}/timestamp.pkl'):\n",
    "                    print(\"this file has already been generated, skipping to next patch\")\n",
    "                    patch_s2 = EOPatch.load(f'{out_path}/lulc_nosample/eopatch_{idx}', lazy_loading=True)\n",
    "                    break\n",
    "                else:\n",
    "                    # Perform execution of defined workflow.\n",
    "                    try:\n",
    "                        results_s2 = workflow.execute(extra_params)\n",
    "                        patch_s2 = list(results_s2.values())[-1]\n",
    "            \n",
    "                        min_valid = np.amin(patch_s2.mask_timeless['VALID_COUNT_SH'])\n",
    "                        min_days = datetime.date(\n",
    "                            int(start_date.split('-')[0]),\n",
    "                            int(start_date.split('-')[1]),\n",
    "                            int(start_date.split('-')[2])\n",
    "                        )\n",
    "            \n",
    "                        max_days = datetime.date(\n",
    "                            int(end_date.split('-')[0]),\n",
    "                            int(end_date.split('-')[1]),\n",
    "                            int(end_date.split('-')[2])\n",
    "                        )\n",
    "                        # Check the smallest temporal sequence of valid data\n",
    "                        # If it is smaller than the number of interpolation/aggregation intervals, reduce the s2cloudless\n",
    "                        # algorithm sensitivity (e.g. likely to occur over bright surfaces like sand or salt).\n",
    "                        # if min_valid < round((max_days - min_days).days / interp_interval):\n",
    "                        #     cloud_threshold = 0.8\n",
    "                        #     print(f'Warning: An abnormally high amount of invalid pixels ({min_valid}) '\n",
    "                        #           f'were detected in certain parts of the eopatch, '\n",
    "                        #           f'retrying with a higher cloud detection threshold...')\n",
    "                        #     continue\n",
    "                        try:\n",
    "                            print(\"number of training pixels in patch:\", np.count_nonzero(patch_s2.data_timeless['SPLIT'] == 1))\n",
    "                            print(\"number of validation pixels in patch:\",\n",
    "                                  np.count_nonzero(patch_s2.data_timeless['SPLIT'] == 2))\n",
    "            \n",
    "                            if gif:\n",
    "                                # Create temporal Giff of the NDVI time series inside the EOPatch\n",
    "                                animated_gif=MakeGif(size=(960,1200))\n",
    "                                animated_gif.add(patch_s2.data['R2'],label=patch_s2.timestamp,band=6)\n",
    "                                animated_gif.save(f'/{out_path}/gif_r2.gif', fps=1)\n",
    "                                \n",
    "                                animated_gif=MakeGif(size=(960,1200))\n",
    "                                animated_gif.add(patch_s2.data['P6'],label=patch_s2.timestamp,band=6)\n",
    "                                animated_gif.save(f'/{out_path}/gif_p6.gif', fps=1)\n",
    "                                \n",
    "                            else:\n",
    "                                # If no training data is available for the given EOPatch,\n",
    "                                # delete the MASK_TIMELESS features LULC and SPLIT\n",
    "                                if np.count_nonzero(patch_s2.mask_timeless['LULC']) == 0:\n",
    "                                    patch_s2.remove_feature(FeatureType.MASK_TIMELESS, 'LULC', )\n",
    "                                    patch_s2.remove_feature(FeatureType.DATA_TIMELESS, 'SPLIT', )\n",
    "                                    if save_choice:\n",
    "                                        patch_s2.save(f'{out_path}/lulc_nosample/eopatch_{idx}',\n",
    "                                                      overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "                                        # Zip the saved EOPatch for additional space saving. Not advised\n",
    "                                        # shutil.make_archive(f'{storage_dir}/{project_name}/lulc_nosample/eopatch_{idx}','zip',\n",
    "                                        #                   f'{storage_dir}/{project_name}/lulc_nosample/eopatch_{idx}')\n",
    "                                        # shutil.rmtree(f'{storage_dir}/{project_name}/lulc_nosample/eopatch_{idx}')\n",
    "                                        del results_s2\n",
    "                                    else:\n",
    "                                        eopatches.append(results_s2[list(results_s2.keys())[-1]])\n",
    "                                else:\n",
    "                                    if save_choice:\n",
    "                                        patch_s2.save(f'{out_path}/lulc_sample/eopatch_{idx}',\n",
    "                                                      overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "                                        del results_s2\n",
    "                                    else:\n",
    "                                        eopatches.append(results_s2[list(results_s2.keys())[-1]])\n",
    "            \n",
    "                        except KeyError:\n",
    "                            if save_choice:\n",
    "                                patch_s2.save(f'{out_path}/lulc_nosample/eopatch_{idx}',\n",
    "                                              overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "                                # Zip the saved EOPatch for additional space saving. Not advised\n",
    "                                # shutil.make_archive(f'{storage_dir}/{project_name}/lulc_nosample/eopatch_{idx}','zip',\n",
    "                                #                   f'{storage_dir}/{project_name}/lulc_nosample/eopatch_{idx}')\n",
    "                                # shutil.rmtree(f'{storage_dir}/{project_name}/lulc_nosample/eopatch_{idx}')\n",
    "                                del results_s2\n",
    "                            else:\n",
    "                                eopatches.append(results_s2[list(results_s2.keys())[-1]])\n",
    "            \n",
    "                        print(\"patch summary:%s\" % patch_s2)\n",
    "                        print(f'eopatch_{idx}')\n",
    "                        print(f'row-{info[\"index_x\"]}')\n",
    "                        print(f'col-{info[\"index_y\"]}')\n",
    "                        print(\"has been processed, moving to next patch\")\n",
    "            \n",
    "                    except MemoryError:\n",
    "                        # TODO: Find a good way to handle MemoryError rather than simply retrying\n",
    "                        # TODO: Add Jira Ticket\n",
    "                        print(f'Sentinel data harvesting failed for eopatch {idx} due to a MemoryError, trying again')\n",
    "                        print(\"Exception in user code:\")\n",
    "                        print('-' * 60)\n",
    "                        traceback.print_exc(file=sys.stdout)\n",
    "                        print('-' * 60)\n",
    "                        attempts += 1\n",
    "                        continue\n",
    "                    except requests.RequestException as err:\n",
    "                        print(f'Sentinel data harvesting failed for eopatch {idx} due to a HTTP Request error '\n",
    "                              f'with code {err.response.status_code}, trying again')\n",
    "                        print(\"Exception in user code:\")\n",
    "                        print('-' * 60)\n",
    "                        traceback.print_exc(file=sys.stdout)\n",
    "                        print('-' * 60)\n",
    "                        attempts += 1\n",
    "                        continue\n",
    "            \n",
    "                    # Last check to see whether there are enough requested S2 datatakes\n",
    "                    # (at least more than half of the number of interpolation intervals in the year)\n",
    "                    # If not the maximum tolerated cloud cover is increased by 0.1 and the EOPatch is reprocessed\n",
    "                    d1 = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "                    d2 = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "                    diff = (d2 - d1).days / (interp_interval * 2)\n",
    "                    print(\"number of dates harvested:\", len(patch_s2.timestamp))\n",
    "                    if len(patch_s2.timestamp) < diff and maxcc < 0.9:\n",
    "                        maxcc = maxcc + 0.1\n",
    "                        # TODO: Consider requesting a longer \"time_interval\" and perform a \"time_difference\" across years\n",
    "                        # TODO: to ensure that the interpolation operation would still produce the same amount of input features\n",
    "                        # TODO: OR Request Sentinel-1 data (but how to fuse S1+S2 to make one single input feature stack?)\n",
    "                        # TODO: add missing JIRA ticket\n",
    "                        attempts += 1\n",
    "                        continue\n",
    "                    break\n",
    "                \n",
    "    return patch_s2\n",
    "\n",
    "resolution = args.resolution\n",
    "hour_diff = args.hour_diff\n",
    "cloud_threshold = args.cloud_threshold\n",
    "gif = args.gif\n",
    "time_range =  args.time_range\n",
    "n_procs = args.n_procs\n",
    "\n",
    "for aoi_idx, bbox_splitter in enumerate(bbox_splitter_list):\n",
    "    range_idx = intersect_aoi(bbox_splitter, trainings[aoi_idx])\n",
    "    load_eopatch_multi = partial(load_eopatch, bbox_splitter, time_range, training_arrays[aoi_idx], \n",
    "                                 split_arrays[aoi_idx], training_vals[aoi_idx], f'{out_path}/{aoi_idx}')\n",
    "    multiprocess(n_procs, range_idx, load_eopatch_multi)\n",
    "\n",
    "# Print any produced output, whether eopatch extents, or gif produced.\n",
    "# Probably best to read from filesystem saved outputs like the gif or the validity raster\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interpolate the loaded EOPatches\n",
    "Finding a new way of aggregating data instead of interpolation would be interesting for two reasons:\n",
    "- Interpolation requires cloud masking, which would we have to work on to apply to LISS-III.\n",
    "- Temporal data aggregation is much less computationally intensive than an interpolation.\n",
    "\n",
    "The best way to go about this would be to:\n",
    "- Define a temporal interval over which to aggregate.\n",
    "- Determine the rule for pixel selection inside the temporal interval (max NDVI value for instance)\n",
    "An article on temporal aggregation (with cloud cover): https://www.researchgate.net/publication/330814279_Evaluating_Combinations_of_Temporally_Aggregated_Sentinel-1_Sentinel-2_and_Landsat_8_for_Land_Cover_Mapping_with_Google_Earth_Engine\n",
    "It would give you an idea of what temporal aggregation means at least. Note that a combination of metrics can be used (e.g. median of one index vs variance of another).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This implementation is an interpolation and not an aggregation, but we could consider switching that.\n",
    "\n",
    "eopatch_sample = Path(str(f'{out_path}/lulc_sample/eopatch_{idx}'))\n",
    "eopatch_nosample = Path(str(f'{out_path}/lulc_nosample/eopatch_{idx}'))\n",
    "eopatch_sampled = Path(str(f'{out_path}/lulc_sampled/eopatch_{idx}'))\n",
    "\n",
    "# TASK FOR CONCATENATION\n",
    "# concatenate = ConcatenateData('FEATURES', ['BANDS', 'NDVI', 'NDWI', 'NORM'])\n",
    "# Add additional collections if more than LIS-III is retrieved\n",
    "concatenate = ConcatenateData('FEATURES', ['R2', 'P6'])\n",
    "\n",
    "# TASK FOR FILTERING OUT TOO CLOUDY SCENES\n",
    "# keep frames with > 80 % valid coverage\n",
    "# valid_data_predicate = ValidDataFractionPredicate(0.8)\n",
    "# filter_task = SimpleFilterTask((FeatureType.MASK, 'IS_VALID'), valid_data_predicate)\n",
    "# valid_lulc_predicate = ValidLULCPredicate()\n",
    "# filter_lulc = SimpleFilterTask((FeatureType.MASK_TIMELESS, 'LULC'), valid_lulc_predicate)\n",
    "\n",
    "# TASK FOR LINEAR INTERPOLATION\n",
    "# linear interpolation of full time-series and date resampling\n",
    "linear_interp = LinearInterpolation(\n",
    "    feature='FEATURES',  # name of field to interpolate\n",
    "    mask_feature=(FeatureType.MASK, 'IS_VALID'),  # mask to be used in interpolation\n",
    "    resample_range=resample_range,  # set the resampling range\n",
    "    parallel=True  # Optimize CPU usage\n",
    ")\n",
    "\n",
    "# Task for performing morphological erosion of specific disk radius around land cover labels of choice\n",
    "# erosion = ErosionTask((FeatureType.MASK_TIMELESS, 'LULC'), disk_radius=disk_radius)\n",
    "erosion = ClassFilterTask((FeatureType.MASK_TIMELESS, 'LULC'), [1, 4, 6, 9], MorphologicalOperations.EROSION,\n",
    "                          struct_elem=MorphologicalStructFactory.get_disk(disk_radius))\n",
    "\n",
    "# Task for performing random sampling of land cover training labels\n",
    "spatial_sampling = PointSamplingTask(\n",
    "    n_samples=n_samples,\n",
    "    ref_mask_feature='LULC',\n",
    "    ref_labels=training_val,\n",
    "    sample_features=[  # tag fields to sample\n",
    "        (FeatureType.DATA, 'FEATURES',),\n",
    "        (FeatureType.MASK, 'IS_VALID',),\n",
    "        (FeatureType.MASK_TIMELESS, 'LULC',),\n",
    "        (FeatureType.DATA_TIMELESS, 'SPLIT',)\n",
    "    ],\n",
    "    even_sampling=even_sampling)\n",
    "\n",
    "# Task to save the resulting EOPatch to the disk\n",
    "os.makedirs(f'{out_path}/lulc_sampled', exist_ok=True)\n",
    "save = SaveToDisk(f'{out_path}/lulc_sampled',\n",
    "                  overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "\n",
    "# Check to verify whether the current EOPatch contains training sample or not\n",
    "if eopatch_sample.is_dir() and not Path(f'{eopatch_sampled}/data/FEATURES_SAMPLED.npy').is_file():\n",
    "    print(\"this patch contains training samples. interpolating and sampling.\")\n",
    "\n",
    "    # Task to move the unaltered features from the old EOPatch to the new EOPatch\n",
    "    move_features = MoveFeature({\n",
    "        FeatureType.MASK_TIMELESS: {'LULC'},\n",
    "        FeatureType.DATA_TIMELESS: {'SPLIT'},\n",
    "        FeatureType.MASK: {'IS_VALID'}})\n",
    "\n",
    "    # Declare locations where to load from/save to\n",
    "    if save_choice:\n",
    "        load = LoadFromDisk(f'{out_path}/lulc_sample', lazy_loading=True)\n",
    "        save_dependency = [Dependency(task=save, inputs=[spatial_sampling])]\n",
    "    else:\n",
    "        load = LoadFromMemory()\n",
    "        save_dependency = []\n",
    "\n",
    "    # Declare workflow with the sequence of tasks\n",
    "    workflow = EOWorkflow(dependencies=[\n",
    "        Dependency(task=load, inputs=[]),\n",
    "        Dependency(task=concatenate, inputs=[load]),\n",
    "        # Dependency(task=filter_task, inputs=[concatenate]),\n",
    "        # Dependency(task=filter_lulc, inputs=[concatenate]),\n",
    "        Dependency(task=linear_interp, inputs=[concatenate]),\n",
    "        # Dependency(task=move_features, inputs=[linear_interp,filter_task]),\n",
    "        Dependency(task=move_features, inputs=[linear_interp, concatenate]),\n",
    "        Dependency(task=erosion, inputs=[move_features]),\n",
    "        Dependency(task=spatial_sampling, inputs=[move_features]),\n",
    "        *save_dependency\n",
    "    ])\n",
    "\n",
    "elif not eopatch_sample.is_dir() and not eopatch_sampled.is_dir():\n",
    "    print(\"this patch does not contain any training samples. interpolating without sampling.\")\n",
    "\n",
    "    # Task to move the unaltered features from the old EOPatch to the new EOPatch\n",
    "    move_features = MoveFeature({\n",
    "        FeatureType.MASK: {'IS_VALID'}})\n",
    "\n",
    "    # Declare locations where to load from/save to\n",
    "    if save_choice:\n",
    "        load = LoadFromDisk(f'{out_path}/lulc_nosample', lazy_loading=True)\n",
    "        save_dependency = [Dependency(task=save, inputs=[move_features])]\n",
    "    else:\n",
    "        load = LoadFromMemory()\n",
    "        save_dependency = []\n",
    "\n",
    "    # Declare workflow with the sequence of tasks\n",
    "    workflow = EOWorkflow(dependencies=[\n",
    "        Dependency(task=load, inputs=[]),\n",
    "        Dependency(task=concatenate, inputs=[load]),\n",
    "        # Dependency(task=filter_task, inputs=[concatenate]),\n",
    "        # Dependency(task=filter_lulc, inputs=[concatenate]),\n",
    "        Dependency(task=linear_interp, inputs=[concatenate]),\n",
    "        # Dependency(task=move_features_ns, inputs=[linear_interp,filter_task]),\n",
    "        Dependency(task=move_features, inputs=[linear_interp]),\n",
    "        *save_dependency\n",
    "    ])\n",
    "\n",
    "# If the reduced EOPatch has already generated, return empty results\n",
    "elif Path(f'{eopatch_sampled}/data/FEATURES_SAMPLED.npy').is_file():\n",
    "    with open(f'{out_path}/lulc_sampled/range_sample.txt', \"a\") as f:\n",
    "        f.write('%s\\n' % idx)\n",
    "    print(\"this patch has already been sampled. skipping to next patch.\")\n",
    "    return\n",
    "else:\n",
    "    print(\"this patch has already been interpolated (patch without samples). skipping to next patch.\")\n",
    "    return\n",
    "\n",
    "# While loop necessary to re-perform workflow with different parameters.\n",
    "# Cases are if a HTTPRequestError (simply retry) or MemoryError (not currently handled) is encountered.\n",
    "attempts = 0\n",
    "while attempts < 5:\n",
    "    # Execute workflow\n",
    "    try:\n",
    "        if save_choice:\n",
    "            result = workflow.execute({\n",
    "                load: {'eopatch_folder': 'eopatch_{}'.format(idx)},\n",
    "                save: {'eopatch_folder': 'eopatch_{}'.format(idx)}\n",
    "            })\n",
    "        else:\n",
    "            result = workflow.execute({\n",
    "                load: {'eopatch': eopatch}\n",
    "            })\n",
    "\n",
    "        print(result[list(result.keys())[-1]])\n",
    "        patch_s2 = list(result.values())[-1]\n",
    "\n",
    "        # Save the EOPatch index to file if it contains training data\n",
    "        if patch_s2.mask_timeless['LULC_SAMPLED'].shape[0] == n_samples:\n",
    "            with open(f'{out_path}/lulc_sampled/range_sample.txt', \"a\") as f:\n",
    "                f.write('%s\\n' % idx)\n",
    "        # If not, remove the below EOPatch layers\n",
    "        else:\n",
    "            patch_s2.remove_feature(FeatureType.MASK_TIMELESS, 'LULC', )\n",
    "            patch_s2.remove_feature(FeatureType.MASK_TIMELESS, 'LULC_SAMPLED', )\n",
    "            patch_s2.remove_feature(FeatureType.DATA_TIMELESS, 'SPLIT', )\n",
    "            patch_s2.remove_feature(FeatureType.DATA_TIMELESS, 'SPLIT_SAMPLED', )\n",
    "            patch_s2.remove_feature(FeatureType.DATA, 'FEATURES_SAMPLED', )\n",
    "            patch_s2.remove_feature(FeatureType.MASK, 'IS_VALID_SAMPLED', )\n",
    "\n",
    "            if save_choice:\n",
    "                patch_s2.save(f'{eopatch_sampled}',\n",
    "                              overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "\n",
    "            # Clean up the folder structure of the EOPatch\n",
    "            if os.path.exists(eopatch_sample):\n",
    "                if not os.path.exists(eopatch_nosample):\n",
    "                    os.mkdir(eopatch_nosample)\n",
    "                for item in os.listdir(eopatch_sample):\n",
    "                    s = os.path.join(eopatch_sample, item)\n",
    "                    d = os.path.join(eopatch_nosample, item)\n",
    "                    if os.path.isdir(s) and not os.path.exists(d):\n",
    "                        shutil.copytree(s, d)\n",
    "                    elif os.path.isfile(s) and not os.path.isfile(d):\n",
    "                        shutil.copy2(s, d)\n",
    "                shutil.rmtree(eopatch_sample)\n",
    "        del result\n",
    "    except MemoryError:\n",
    "        # TODO: Find a good way to handle MemoryError rather than simply retrying\n",
    "        # TODO: Add Jira Ticket\n",
    "        print(f'Interpolation failed for eopatch {idx} due to a MemoryError, trying again')\n",
    "        print(\"Exception in user code:\")\n",
    "        print('-'*60)\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        print('-'*60)\n",
    "        attempts += 1\n",
    "        continue\n",
    "    except OSError:\n",
    "        # TODO: Find a good way to handle MemoryError rather than simply retrying\n",
    "        # TODO: Add Jira Ticket\n",
    "        print(f'Interpolation failed for eopatch {idx} due to an OSError, trying again')\n",
    "        print(\"Exception in user code:\")\n",
    "        print('-'*60)\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        print('-'*60)\n",
    "        attempts += 1\n",
    "        continue\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train a RF model for the respective AOIs\n",
    "The implementation is a simple random forest ensemble, but if we have time we could investigate in the following:\n",
    "- RNN or CNN using tensorflow (I don't really see it happening as we would need to recollect new training data).\n",
    "- use SHAP ( https://github.com/slundberg/shap ) to perform a ML model explainability analysis. \n",
    "It doesn't directly help in getting the required outputs for the challenge, \n",
    "but would be interesting to understand correlation between input features and output.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eopatches, range_sample = clean_training_data(range_sample, out_path)\n",
    "\n",
    "range_sample, features_train, features_test, labels_train, labels_test, labels_unique, p1, t1, w1, h1, f1 = \\\n",
    "                                                                                            craft_input_features(\n",
    "                                                                                            eopatches,\n",
    "                                                                                            range_sample,\n",
    "                                                                                            lulc_classes,\n",
    "                                                                                            out_path)\n",
    "\n",
    "# Set up the model\n",
    "if model_path is not None:\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "    boosting_type='rf',\n",
    "    objective='multiclass',\n",
    "    num_class=len(labels_unique),\n",
    "    metric='multi_logloss',\n",
    "    bagging_freq=1,\n",
    "    bagging_fraction=0.632,\n",
    "    feature_fraction=0.632\n",
    "    # class_weight={0:0.1,1:0.15,2:0.05,3:0.05,4:0.15,5:0.15,6:0.05,7:0.15,8:0.15}\n",
    "    # n_jobs=8\n",
    ")\n",
    "# train the model\n",
    "model.fit(features_train, labels_train)\n",
    "\n",
    "start_date = time_interval[0]\n",
    "end_date = time_interval[-1]\n",
    "if not os.path.isdir(f'{out_path}/model'):\n",
    "    os.makedirs(f'{out_path}/model', exist_ok=True)\n",
    "joblib.dump(model, '{0}/model/{1}_{2}_{3}_{4}_{5}.pkl'\n",
    "            .format(out_path, classifier, start_date, end_date,\n",
    "                    interp_interval, datetime.datetime.now().strftime(\"%m.%d.%Y-%H:%M\")))\n",
    "joblib.dump(labels_unique, '{0}/model/labels_unique_{1}_{2}_{3}_{4}.pkl'\n",
    "            .format(out_path, start_date, end_date,\n",
    "                    interp_interval, datetime.datetime.now().strftime(\"%m.%d.%Y-%H:%M\")))\n",
    "\n",
    "# Plot test results and save them to ./test folder\n",
    "#_test_model(model, eopatches, p1, t1, w1, h1, f1, features_test, labels_train, labels_test, lulc_classes, out_path)\n",
    "\n",
    "# Perform a shap ML explainability analysis and save them to ./shap folder\n",
    "#if shap:\n",
    "#    shap_explainer(model, features_train, out_path)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot test results\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict the test labels\n",
    "plabels_test = model.predict(features_test)\n",
    "plabels_test = plabels_test.reshape(plabels_test.shape[0], 1)\n",
    "\n",
    "class_labels = [cl for cl in lulc_classes.values()]\n",
    "class_names = [cn for cn in lulc_classes.keys()]\n",
    "\n",
    "# Reference colormap things\n",
    "lulc_cmap = mpl.colors.ListedColormap([entry.color for entry in LULC])\n",
    "\n",
    "print('Classification accuracy {:.1f}%'.format(100 * metrics.accuracy_score(labels_test, plabels_test)))\n",
    "print(\n",
    "    'Classification F1-score {:.1f}%'.format(100 * metrics.f1_score(labels_test, plabels_test, average='weighted')))\n",
    "\n",
    "f1_scores = metrics.f1_score(labels_test, plabels_test, labels=class_labels, average=None)\n",
    "recall = metrics.recall_score(labels_test, plabels_test, labels=class_labels, average=None)\n",
    "precision = metrics.precision_score(labels_test, plabels_test, labels=class_labels, average=None)\n",
    "\n",
    "t = open(f'{out_path}/validation/test_confusion_matrix.txt', 'w+')\n",
    "print('             Class              =  F1  | Recall | Precision')\n",
    "t.write('             Class              =  F1  | Recall | Precision\\n')\n",
    "print('         --------------------------------------------------')\n",
    "t.write('         --------------------------------------------------\\n')\n",
    "for idx, lulctype in zip(range(len(class_labels)), class_names):\n",
    "    print('         * {0:20s} = {1:2.1f} |  {2:2.1f}  | {3:2.1f}'.format(lulctype,\n",
    "                                                                         f1_scores[idx] * 100,\n",
    "                                                                         recall[idx] * 100,\n",
    "                                                                         precision[idx] * 100))\n",
    "    t.write('         * {0:20s} = {1:2.1f} |  {2:2.1f}  | {3:2.1f}\\n'.format(lulctype,\n",
    "                                                                             f1_scores[idx] * 100,\n",
    "                                                                             recall[idx] * 100,\n",
    "                                                                             precision[idx] * 100))\n",
    "t.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig1 = plt.figure(figsize=(20,20))\n",
    "\n",
    "class_names_test = [class_names[label] for label in np.unique(labels_test)]\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "conf_matrix_gbm = metrics.confusion_matrix(labels_test, plabels_test)\n",
    "plot_confusion_matrix(conf_matrix_gbm,\n",
    "                      classes=class_names_test,\n",
    "                      normalize=True,\n",
    "                      ylabel='Truth (RABA)',\n",
    "                      xlabel='Predicted (GBM)',\n",
    "                      title='Confusion matrix')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "conf_matrix_gbm = metrics.confusion_matrix(plabels_test, labels_test)\n",
    "plot_confusion_matrix(conf_matrix_gbm,\n",
    "                      classes=class_names_test,\n",
    "                      normalize=True,\n",
    "                      xlabel='Truth (RABA)',\n",
    "                      ylabel='Predicted (GBM)',\n",
    "                      title='Transposed Confusion matrix')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig2 = plt.figure(figsize=(20,5))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training Data Distribution Across Classes\n",
    "label_ids, label_counts = np.unique(labels_train, return_counts=True)\n",
    "\n",
    "plt.barh(range(len(label_ids)),label_counts)\n",
    "plt.yticks(range(len(label_ids)), [class_names[i] for i in label_ids], fontsize=20)\n",
    "plt.xticks(fontsize=20, rotation=45)\n",
    "plt.title(f'Training Data Abundance Distribution across classes', fontsize=16, y=1.2)\n",
    "plt.suptitle(f'Total sample size: {np.sum(label_counts)}',fontsize=14)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "\n",
    "#Calculate precision and recall rates.\n",
    "class_labels = np.unique(np.hstack([labels_test,labels_train]))\n",
    "print(class_names)\n",
    "\n",
    "scores_test = model.predict_proba(features_test)\n",
    "labels_binarized = preprocessing.label_binarize(labels_test, classes=class_labels)\n",
    "plot_colors = ['xkcd:darkgreen', 'xkcd:lime','xkcd:tan', 'orange','xkcd:beige','crimson','xkcd:azure', 'xkcd:lavender','xkcd:lightblue'] #'white','xkcd:lavender', 'black'\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for idx,lbl in enumerate(class_labels):\n",
    "    fpr[idx], tpr[idx], _ = metrics.roc_curve(labels_binarized[:, idx], scores_test[:, idx])\n",
    "    roc_auc[idx] = metrics.auc(fpr[idx], tpr[idx])\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "absent_value = []\n",
    "for idx,lbl in enumerate(class_labels):\n",
    "    if np.isnan(roc_auc[idx]):\n",
    "        continue\n",
    "    try:\n",
    "        plt.plot(fpr[idx], tpr[idx], color=plot_colors[idx],lw=2, label=class_names[lbl]+' (%0.5f)' % roc_auc[idx])\n",
    "    except:\n",
    "        absent_value.append(lbl)\n",
    "        continue\n",
    "        \n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 0.2])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=20)\n",
    "plt.ylabel('True Positive Rate', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(f'Receiver operating characteristic for {project_name}', fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={'size':15})\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Feature Importance\n",
    "\n",
    "# names of features\n",
    "fnames = ['B2','B3','B4','B8','B11','B12','NDVI','NDWI','NORM']\n",
    "\n",
    "# get feature importances and reshape them to dates and features\n",
    "z = np.zeros(t1*f1)\n",
    "z = model.feature_importances_\n",
    "z = z.reshape((t1,f1))\n",
    "\n",
    "fig3 = plt.figure(figsize=(20,18))\n",
    "ax = plt.gca()\n",
    "\n",
    "# plot the importances\n",
    "im = ax.imshow(z,aspect=0.5)\n",
    "plt.xticks(range(len(fnames)), fnames, rotation = 45, fontsize=20)\n",
    "plt.yticks(range(t1), ['T{}'.format(i) for i in range(t1)], fontsize=20)\n",
    "\n",
    "cax = fig3.add_axes([0.82, 0.125, 0.04, 0.755])\n",
    "plt.colorbar(im, cax=cax)\n",
    "plt.title(f'feature importance per date for {project_name}', fontsize=20, x=-7)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot most important band/date combination\n",
    "\n",
    "z_max = unravel_index(np.argmax(z), z.shape)\n",
    "print(z_max)\n",
    "z_min = unravel_index(np.argmin(z[z>0]), z.shape)\n",
    "print(z_min)\n",
    "\n",
    "bsub_tsub = np.swapaxes(np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in eopatches[range_sample]]), 1, 3)[...,z_min[0],z_min[1]].reshape(p1*h1*w1)\n",
    "bopt_tsub = np.swapaxes(np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in eopatches[range_sample]]), 1, 3)[...,z_max[0],z_min[1]].reshape(p1*h1*w1)\n",
    "bsub_topt = np.swapaxes(np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in eopatches[range_sample]]), 1, 3)[...,z_min[0],z_max[1]].reshape(p1*h1*w1)\n",
    "bopt_topt = np.swapaxes(np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in eopatches[range_sample]]), 1, 3)[...,z_max[0],z_max[1]].reshape(p1*h1*w1)\n",
    "labels = np.array([eopatch.mask_timeless['LULC_SAMPLED'] for eopatch in eopatches[range_sample]]).reshape(p1*h1*w1*1)\n",
    "\n",
    "# remove nans\n",
    "mask = np.any([np.isnan(bsub_tsub), np.isnan(bopt_tsub), np.isnan(bsub_topt), np.isnan(bopt_topt), labels==0],axis=0)\n",
    "bsub_tsub, bopt_tsub, bsub_topt, bopt_topt, labels = [array[~mask] for array in [bsub_tsub, bopt_tsub, bsub_topt, bopt_topt, labels]]\n",
    "\n",
    "fig4 = plt.figure(figsize=(20,20))\n",
    "\n",
    "plot_labels = np.unique(labels)\n",
    "plot_colors = lulc_cmap.colors\n",
    "plot_colors = ['xkcd:darkgreen', 'xkcd:lime','xkcd:tan', 'orange','xkcd:beige','crimson','xkcd:azure', 'xkcd:lavender','xkcd:lightblue'] #'white','xkcd:lavender', 'black'\n",
    "print(plot_labels)\n",
    "print(plot_colors)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist([bsub_topt[labels == i] for i in plot_labels],100,(-0.4, 0.8),histtype='step',\n",
    "         color=[plot_colors[i] for i in range(len(plot_labels))],\n",
    "         label=[class_names[i] for i in plot_labels],\n",
    "         )\n",
    "plt.title(f'Most important band at least optimal date T{z_min[0]}', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(f'{fnames[z_max[1]]}', fontsize=20)\n",
    "plt.legend(loc=1, prop={'size':15})\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist([bopt_topt[labels == i] for i in plot_labels],100,(-0.4, 0.8),histtype='step',\n",
    "         color=[plot_colors[i] for i in range(len(plot_labels))],\n",
    "         )\n",
    "plt.title(f'Most important band at most optimal date T{z_max[0]}', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(f'{fnames[z_max[1]]}', fontsize=20)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(f'Least important band at least optimal date T{z_min[0]}', fontsize=20)\n",
    "plt.hist([bsub_tsub[labels == i] for i in plot_labels],100,(0.1, 0.7),histtype='step',\n",
    "         color=[plot_colors[i] for i in range(len(plot_labels))],\n",
    "         )\n",
    "plt.xticks(fontsize=20)#     plt.yticks(fontsize=20)\n",
    "plt.xlabel(f'{fnames[z_min[1]]}', fontsize=20)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(f'Least important band at most optimal date T{z_max[0]}', fontsize=20)\n",
    "plt.hist([bopt_tsub[labels == i] for i in plot_labels],100,(0.1, 0.7),histtype='step',\n",
    "         color=[plot_colors[i] for i in range(len(plot_labels))],\n",
    "         )\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(f'{fnames[z_min[1]]}', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "pbar = tqdm(total=len(bbox_splitter.bbox_list))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict EOPatches using trained model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "info = bbox_splitter.info_list[idx]\n",
    "\n",
    "os.makedirs(f'{out_path}/lulc_pred', exist_ok=True)\n",
    "\n",
    "tiff_pred = f'pred_eopatch_{idx}_row-{info[\"index_x\"]}_col-{info[\"index_y\"]}.tiff'\n",
    "pred_output = PredictPatch(model, len(labels_unique), proba, shap)\n",
    "export_pred = ExportToTiff((FeatureType.DATA_TIMELESS, 'PRED'),\n",
    "                           image_dtype=np.uint8, no_data_value=0, date_indices=[0], band_indices=[0])\n",
    "\n",
    "if not os.path.isdir(f'{out_path}/logs'):\n",
    "    os.makedirs(f'{out_path}/logs')\n",
    "\n",
    "range_missing = open(f'{out_path}/logs/missing_eopatches.txt', 'w+')\n",
    "if os.path.isdir(f'{out_path}/lulc_sampled/eopatch_{idx}'):\n",
    "    load = LoadFromDisk(f'{out_path}/lulc_sampled')\n",
    "    save = SaveToDisk(f'{out_path}/lulc_sampled',\n",
    "                      overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "else:\n",
    "    # TODO: How to send these EOPatches to the back of the job queue for reprocessing?\n",
    "    print(f'EOPatch {idx} is missing! Logging it for reprocessing...')\n",
    "    range_missing.write(f'{idx}\\n')\n",
    "    return\n",
    "\n",
    "if proba is True:\n",
    "    export_proba = ExportToTiff((FeatureType.DATA_TIMELESS, 'PRED_PROBA'),\n",
    "                                image_dtype=np.float32, band_indices=(0, 8), no_data_value=0, date_indices=[0])\n",
    "    workflow_pred = LinearWorkflow(load,\n",
    "                                   pred_output,\n",
    "                                   save,\n",
    "                                   export_pred,\n",
    "                                   export_proba\n",
    "                                   )\n",
    "\n",
    "    tiff_proba = f'proba_eopatch_{idx}_row-{info[\"index_x\"]}_col-{info[\"index_y\"]}.tiff'\n",
    "    extra_param[export_proba] = {'filename': f'{out_path}/lulc_pred/{tiff_proba}'}\n",
    "else:\n",
    "    workflow_pred = LinearWorkflow(load,\n",
    "                                   pred_output,\n",
    "                                   save,\n",
    "                                   export_pred\n",
    "                                   )\n",
    "\n",
    "if os.path.isfile(f'{out_path}/lulc_pred/{tiff_pred}'):\n",
    "    print(\"this patch has already been predicted. skipping to next patch.\")\n",
    "    return\n",
    "else:\n",
    "    attempts = 0\n",
    "    while attempts < 5:\n",
    "        try:\n",
    "            result_pred = workflow_pred.execute(\n",
    "                {load: {'eopatch_folder': f'eopatch_{idx}'},\n",
    "                 save: {'eopatch_folder': f'eopatch_{idx}'},\n",
    "                 export_pred: {'filename': f'{out_path}/lulc_pred/{tiff_pred}'}\n",
    "                 })\n",
    "            print(result_pred[list(result_pred.keys())[-1]])\n",
    "            patch_s2 = list(result_pred.values())[-1]\n",
    "        except:\n",
    "            print(f'Prediction for eopatch {idx} has failed, trying again...')\n",
    "            print(\"Exception in user code:\")\n",
    "            print('-' * 60)\n",
    "            traceback.print_exc(file=sys.stdout)\n",
    "            print('-' * 60)\n",
    "            attempts += 1\n",
    "            continue\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot a web map. \n",
    "Some examples of implementation using ipywidget and ipyleaflet. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Color mapping of the land cover classes to display.\n",
    "newcolors = np.zeros((12,4))\n",
    "newcolors[0,:] = np.asarray([0,0,0,0])\n",
    "newcolors[1,:] = np.asarray([183, 219, 177, 255])\n",
    "newcolors[2,:] = np.asarray([0,0,0,0])\n",
    "newcolors[3,:] = np.asarray([149, 224, 145, 255])\n",
    "newcolors[4,:] = np.asarray([255, 223, 145, 255])\n",
    "newcolors[5,:] = np.asarray([0,0,0,0])\n",
    "newcolors[6,:] = np.asarray([226, 193, 115, 255])\n",
    "newcolors[7,:] = np.asarray([255, 244, 150, 255])\n",
    "newcolors[8,:] = np.asarray([255, 150, 150, 255])\n",
    "newcolors[9,:] = np.asarray([150, 171, 255, 255])\n",
    "newcolors[10,:] = np.asarray([209, 183, 255, 255])\n",
    "newcolors[11,:] = np.asarray([183, 255, 253, 255])\n",
    "\n",
    "print(newcolors/255)\n",
    "newcmp = ListedColormap(newcolors/255)\n",
    "print(newcmp)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a map using Stamen Terrain, centered on study area with set zoom level\n",
    "m = Map(center=(37, -120), zoom=0, basemap=basemaps.Stamen.Toner)\n",
    "m\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sc = Sidecar(title='California Map')\n",
    "with sc:\n",
    "    display(m)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " # Iteratively display the predicted tiles as they become available in folder structure\n",
    "    for tiff in glob.glob(\"/mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/val_pred_sh_eopatch_*local.tiff\"):\n",
    "    out_path = \"/mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/folium/\"+tiff[:-5].split('/')[-1]+\"_4326.tiff\"\n",
    "    with rio.open(out_path) as src:\n",
    "        boundary = src.bounds\n",
    "    \n",
    "    m.add_layer(ImageOverlay(url=\"http://172.29.254.183:8000/\"+tiff[:-5].split('/')[-1]+\"_4326.png\", bounds=((boundary[1], boundary[0]), \n",
    "                                             (boundary[3], boundary[2])), opacity=1))\n",
    "    m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Add a local tile server\n",
    "os.chdir(\"/mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/\")\n",
    "m.add_layer(TileLayer(url=\"http://172.29.254.183:8000/{z}/{x}/{y}.png\"))\n",
    "m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for tiff in glob.glob(\"/mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/val_pred_sh_eopatch_*local.tiff\"):\n",
    "    # Create variables for destination coordinate system and the name of the projected raster\n",
    "    src_crs = 'EPSG:32719' \n",
    "    dst_crs = 'EPSG:4326' \n",
    "    out_vrt = \"/mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/folium/\"+tiff[:-5].split('/')[-1]+\"_col1.tiff\"\n",
    "    out_vrt1 = \"/mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/folium/\"+tiff[:-5].split('/')[-1]+\"_col.tiff\"\n",
    "    out_path = \"/mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/folium/\"+tiff[:-5].split('/')[-1]+\"_4326.tiff\"\n",
    "    out_jpg = \"/mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/folium/\"+tiff[:-5].split('/')[-1]+\"_4326.png\"\n",
    "    out_tms = \"/mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/folium/\"+tiff[:-5].split('/')[-1]+\"_4326\"\n",
    "    #print(out_path)\n",
    "    \n",
    "    cmd = 'gdaldem color-relief %s /mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/col.txt %s -of Gtiff'%(tiff,out_vrt)\n",
    "    !{cmd}\n",
    "    \"\"\"cmd1 = 'gdalwarp -of GTiff -overwrite -s_srs %s -t_srs %s %s %s'%(src_crs, dst_crs, out_vrt, out_vrt1)\n",
    "    !{cmd1}\n",
    "    os.remove(out_vrt)\n",
    "    cmd2 = 'gdal_translate -of JPEG %s %s'%(out_vrt1,out_path)\n",
    "    !{cmd2}\"\"\"\n",
    "    \n",
    "    #subprocess.call([\"gdaldem\",\"color-relief\", tiff, \"/mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/col.txt\",out_vrt,\"-of\" ,\"VRT\"])\n",
    "    #subprocess.call([\"gdalwarp\",\"-of\", \"JPEG\", \"-s_srs\",src_crs,\"-t_srs\",dst_crs, out_vrt, out_path])\n",
    "    \n",
    "    # Use rasterio package as rio to open and project the raster\n",
    "    with rio.open(out_vrt) as src:\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({\n",
    "            'crs': dst_crs,\n",
    "            'transform': transform,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "\n",
    "        # Use rasterio package as rio to write out the new projected raster\n",
    "        # Code uses loop to account for multi-band rasters\n",
    "        with rio.open(out_path, 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                source=rio.band(src, i),\n",
    "                destination=rio.band(dst, i),\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=transform,\n",
    "                dst_crs=dst_crs,\n",
    "                resampling=Resampling.nearest)\n",
    "    # Use rasterio to import the reprojected data as img\n",
    "    with rio.open(out_path) as src:\n",
    "        boundary = src.bounds\n",
    "        #print(boundary)\n",
    "        #img = src.read()\n",
    "        #nodata = src.nodata\n",
    "    \n",
    "    cmd2 = \"convert %s -transparent black -fuzz 11%% %s\"%(out_path, out_jpg)\n",
    "    !{cmd2}\n",
    "\n",
    "    # Overlay raster called img using add_child() function (opacity and bounding box set)\n",
    "    #m.add_child( folium.raster_layers.ImageOverlay(img[0], opacity=1, colormap=newcmp,\n",
    "    #                                 bounds =[[boundary[1], boundary[0]], [boundary[3], boundary[2]]]))\n",
    "    m.add_layer(ImageOverlay(url=\"http://172.29.254.183:8000/\"+tiff[:-5].split('/')[-1]+\"_4326.png\", bounds=((boundary[1], boundary[0]), \n",
    "                                             (boundary[3], boundary[2])), opacity=1))\n",
    "\n",
    "    m\n",
    "    \n",
    "m.save(outfile= \"/mnt/1t-drive/eopatch-L1C/nam_usa_uca/lulc_pred/folium/test.html\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}